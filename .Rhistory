}
sub_soft_fail <- filter(subs,softs >= 2) #identify those who meet more than 2 soft cutoffs
subs_to_exclude <- unique(c(sub_hard_fail$id,sub_soft_fail$id)) #get subjects who failed either set of criteria
#clean data
trials <- trials %>% filter(!(id %in% subs_to_exclude))
subs <- subs %>% filter(!(id %in% subs_to_exclude))
length(subs_to_exclude)/(nrow(subs)+length(subs_to_exclude)) #get percent excluded
trials <- add_sub_indices(trials) #add subject indices to the df. These will match the indices used in Stan.
trials <- add_probe_number(trials,newcol="vrat_number",val_col="val_rat") #add valence rating number
trials <- add_probe_number(trials,newcol="prat_number",val_col="prob_rat") #add probability rating number
# Create mean-centered block and trial numbers
trials <- mutate(trials, bl_cent = block - mean(block), tr_cent = trial_blk - mean(trial_blk))
# Add fractal indexes
trials <- trials %>% mutate(frac_ix = case_when(
frac_img == 1 ~ 1,
frac_img == 9 ~ 2,
frac_img == 3 ~ 3,
frac_img == 5 ~ 4,
)) %>%
mutate(alt_frac_ix = case_when(
frac_ix == 1 ~ 2,
frac_ix == 2 ~ 1,
frac_ix == 3 ~ 4,
frac_ix == 4 ~ 3
))
sum(subs$sig_emot == "Yes")/nrow(subs)
subs$age <- as.numeric(subs$age)
mean(subs$age)
median(subs$age)
sd(subs$age)
sum(subs$gender=="Male")
sum(subs$gender=="Female")
sum(subs$gender=="Non-binary")
view(trials)
source("~/projects/RVA_3/code/functions/rva3_utilities.R", echo=TRUE)
prereg <- fit_stan_model(stan_file=paste0(stan_model_dir,"prereg_rva3.stan"),
model_out_dir=model_out_dir,
raw_data=trials,
study = "rva3")
n_t
n_s <- length(unique(trials$id)) #get number of subjects
n_f <- max(trials$frac_ix)
out_size <- abs(trials$outcome[1]) # Absolute value of outcomes
rew <- sub_by_trial_vec_list(trials,"outcome") # Outcome on each trial
rew_hid <- sub_by_trial_vec_list(trials,"rew_hid") # Outcome on each trial
bl_cent <- sub_by_trial_vec_list(trials,"bl_cent") # Block number, mean-centered
tr_cent <- sub_by_trial_vec_list(trials,"tr_cent") # Trial number, mean-centered
frac <- sub_by_trial_matrix(trials,"frac_ix") # Fractal index on each trial
#valence rating data
val_rat_num <- sub_by_trial_matrix(trials,"vrat_number")
n_vrat <- max(trials$vrat_number)
val_rat_trials <- filter(trials,vrat_number != 0)
val_rat <- val_rat_trials$val_rat
#probability rating data
prob_rat_num <- sub_by_trial_matrix(trials,"prat_number")
n_prat <- max(trials$prat_number)
prob_rat_trials <- filter(trials,prat_number != 0)
prob_rat <- prob_rat_trials$prob_rat
n_t
n_s
n_f
out_size
rew
rew_hid
frac
val_rat_num
144-96
prob_rat_num
n_prat
n_vrat
bl_cent
tr_cent
source("~/projects/RVA_3/code/functions/rva3_utilities.R")
prereg <- fit_stan_model(stan_file=paste0(stan_model_dir,"prereg_rva3.stan"),
model_out_dir=model_out_dir,
raw_data=trials,
study = "rva3")
view(filt_sum(prereg$sum,"mu"))
view(filt_sum(prereg$sum,"sigma"))
library(qualtRics)
library(tidyverse)
library(ddpcr)
library(tidyverse)
library(sigmoid)
##SET MANUALLY
path_to_project_directory <- "~/projects/RVA_3/"
path_to_s22 <- "~/projects/spring_2022_study/"
path_to_s22fu <- "~/projects/s22_follow_up/"
path_to_rva <- "~/projects/RVA/"
ids_to_exclude <- c("6398dc9eee4c333e1712e777") #Ps whose data you don't want to analyze even if it looks good
##############
#clear out results from old analyses
system("mv /Users/dp/projects/RVA_3/analysis_data/*.csv /Users/dp/projects/RVA_3/analysis_data/old_analysis_data") #move any CSV files in this folder to old_analysis_data folder
system("rm -rf /Users/dp/projects/RVA_3/analysis_data/qc_plots/trial_level_vars/*") #clear the folder containing trial-level plots
system("rm -f /Users/dp/projects/RVA_3/analysis_data/qc_plots/sub_level_variables.pdf") #remove old subject level variable plot
#read in necessary functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22fu,"code/functions/s22fu_utilities.R"))
source(paste0(path_to_rva,"code/functions/rva_utilities.R"))
source(paste0(path_to_project_directory,"code/functions/rva3_wrangle_psychopy_data.R"))
csvs_to_analyze <- get_csvs_to_analyze(path_to_project_directory,ids_to_exclude,first_time="2025-03-01") #get the usable CSVs to analyze
all_data <- lapply(csvs_to_analyze, rva3_wrangle_psychopy_data) #reformats each CSV, turning it into a long dataset usable for analysis, and adds all variables of interest that can be created from the raw data alone.
#Returns a list - one element for each subject - where each element is itself a list containing dfs with the trial-level data and subject-level data
trials_list <- lapply(all_data, function(l) l[[1]]) #get a list of the trial-level dfs only
trial_level_data <- do.call(rbind,trials_list) #stack trial dfs into one big df
sub_list <- lapply(all_data, function(l) l[[2]]) #get a list of the subject-level dfs only
sub_level_data <- do.call(rbind,sub_list) #stack them into one big data frame
#append the Yes/No answer from qualtrics to this data
sub_level_data <- qualtrics_append(sub_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
trial_level_data <- qualtrics_append(trial_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
#write both to CSVs
date_time <- Sys.time() %>% chartr(" ","_",.) %>% chartr(":","_",.) #grab the date and time, reformatting ':' and '-' to  '_' so you can label the files with it
write.csv(trial_level_data,paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#write subject-level data
write.csv(sub_level_data,paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
library(qualtRics)
library(tidyverse)
library(ddpcr)
library(tidyverse)
library(sigmoid)
##SET MANUALLY
path_to_project_directory <- "~/projects/RVA_3/"
path_to_s22 <- "~/projects/spring_2022_study/"
path_to_s22fu <- "~/projects/s22_follow_up/"
path_to_rva <- "~/projects/RVA/"
ids_to_exclude <- c("6398dc9eee4c333e1712e777") #Ps whose data you don't want to analyze even if it looks good
##############
#clear out results from old analyses
system("mv /Users/dp/projects/RVA_3/analysis_data/*.csv /Users/dp/projects/RVA_3/analysis_data/old_analysis_data") #move any CSV files in this folder to old_analysis_data folder
system("rm -rf /Users/dp/projects/RVA_3/analysis_data/qc_plots/trial_level_vars/*") #clear the folder containing trial-level plots
system("rm -f /Users/dp/projects/RVA_3/analysis_data/qc_plots/sub_level_variables.pdf") #remove old subject level variable plot
#read in necessary functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22fu,"code/functions/s22fu_utilities.R"))
source(paste0(path_to_rva,"code/functions/rva_utilities.R"))
source(paste0(path_to_project_directory,"code/functions/rva3_wrangle_psychopy_data.R"))
csvs_to_analyze <- get_csvs_to_analyze(path_to_project_directory,ids_to_exclude,first_time="2025-03-05") #get the usable CSVs to analyze
all_data <- lapply(csvs_to_analyze, rva3_wrangle_psychopy_data) #reformats each CSV, turning it into a long dataset usable for analysis, and adds all variables of interest that can be created from the raw data alone.
#Returns a list - one element for each subject - where each element is itself a list containing dfs with the trial-level data and subject-level data
trials_list <- lapply(all_data, function(l) l[[1]]) #get a list of the trial-level dfs only
trial_level_data <- do.call(rbind,trials_list) #stack trial dfs into one big df
sub_list <- lapply(all_data, function(l) l[[2]]) #get a list of the subject-level dfs only
sub_level_data <- do.call(rbind,sub_list) #stack them into one big data frame
#append the Yes/No answer from qualtrics to this data
sub_level_data <- qualtrics_append(sub_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
trial_level_data <- qualtrics_append(trial_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
#write both to CSVs
date_time <- Sys.time() %>% chartr(" ","_",.) %>% chartr(":","_",.) #grab the date and time, reformatting ':' and '-' to  '_' so you can label the files with it
write.csv(trial_level_data,paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#write subject-level data
write.csv(sub_level_data,paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#Create plot grids of select variables for quality checking. These are saved to the qc_plots folder
tl_hists <- c("val_rat_rt","prob_rat_rt","val_rat","prob_rat") #trial level variables to plot
plot_trial_level_vars(trial_level_data,tl_hists,path_to_project_directory) #create and save plot grids
sub_path <- "/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_16_06_54.059091.csv"
atcheck_review(sub_path,"2025-03-05",min_passed = 3)
source("~/projects/ARL_bv/code/functions/arlbv_utilities.R", echo=TRUE)
atcheck_review(sub_path,"2025-03-05",min_passed = 3)
source("~/projects/ARL_bv/code/functions/arlbv_utilities.R", echo=TRUE)
atcheck_review(sub_path,"2025-03-05",min_passed = 3)
atcheck_review(sub_path,"2025-03-05",min_passed = 2)
temp <- atcheck_review(sub_path,"2025-03-05",min_passed = 2)
prolific_bp(sub_path,date_min="2025-03-05",temp$approve_ids)
source("~/projects/ARL_bv/code/functions/arlbv_utilities.R", echo=TRUE)
prolific_bp(sub_path,date_min="2025-03-05",temp$approve_ids)
sub <- sub %>% filter(date >= date_min & date <= date_max) #grab subjects within the date range
date_min
date <= date_max
date_max
date_min
prolific_bp(sub_path,date_min="2025-03-05",ids=temp$approve_ids)
source("~/projects/ARL_bv/code/functions/arlbv_utilities.R", echo=TRUE)
prolific_bp(sub_path,date_min="2025-03-05",ids=temp$approve_ids)
#This is the master script for initial data analysis steps
#It identifies data to use, wrangles it into a usable form, creates additional variables based on this data, and creates plots of certain variables for quality-checking.
library(qualtRics)
library(tidyverse)
library(ddpcr)
library(tidyverse)
library(sigmoid)
##SET MANUALLY
path_to_project_directory <- "~/projects/RVA_3/"
path_to_s22 <- "~/projects/spring_2022_study/"
path_to_s22fu <- "~/projects/s22_follow_up/"
path_to_rva <- "~/projects/RVA/"
ids_to_exclude <- c("6398dc9eee4c333e1712e777") #Ps whose data you don't want to analyze even if it looks good
##############
#clear out results from old analyses
system("mv /Users/dp/projects/RVA_3/analysis_data/*.csv /Users/dp/projects/RVA_3/analysis_data/old_analysis_data") #move any CSV files in this folder to old_analysis_data folder
system("rm -rf /Users/dp/projects/RVA_3/analysis_data/qc_plots/trial_level_vars/*") #clear the folder containing trial-level plots
system("rm -f /Users/dp/projects/RVA_3/analysis_data/qc_plots/sub_level_variables.pdf") #remove old subject level variable plot
#read in necessary functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22fu,"code/functions/s22fu_utilities.R"))
source(paste0(path_to_rva,"code/functions/rva_utilities.R"))
source(paste0(path_to_project_directory,"code/functions/rva3_wrangle_psychopy_data.R"))
csvs_to_analyze <- get_csvs_to_analyze(path_to_project_directory,ids_to_exclude,first_time="2025-03-05") #get the usable CSVs to analyze
all_data <- lapply(csvs_to_analyze, rva3_wrangle_psychopy_data) #reformats each CSV, turning it into a long dataset usable for analysis, and adds all variables of interest that can be created from the raw data alone.
#Returns a list - one element for each subject - where each element is itself a list containing dfs with the trial-level data and subject-level data
trials_list <- lapply(all_data, function(l) l[[1]]) #get a list of the trial-level dfs only
trial_level_data <- do.call(rbind,trials_list) #stack trial dfs into one big df
sub_list <- lapply(all_data, function(l) l[[2]]) #get a list of the subject-level dfs only
sub_level_data <- do.call(rbind,sub_list) #stack them into one big data frame
#append the Yes/No answer from qualtrics to this data
sub_level_data <- qualtrics_append(sub_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
trial_level_data <- qualtrics_append(trial_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
#write both to CSVs
date_time <- Sys.time() %>% chartr(" ","_",.) %>% chartr(":","_",.) #grab the date and time, reformatting ':' and '-' to  '_' so you can label the files with it
write.csv(trial_level_data,paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#write subject-level data
write.csv(sub_level_data,paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#Create plot grids of select variables for quality checking. These are saved to the qc_plots folder
tl_hists <- c("val_rat_rt","prob_rat_rt","val_rat","prob_rat") #trial level variables to plot
plot_trial_level_vars(trial_level_data,tl_hists,path_to_project_directory) #create and save plot grids
sl_hists <- c("answers_incorrect","instruct_keypress",
"feed_check_passed","aff_check_passed","worth_check_passed","att_checks_passed",
"probrat_skipped_percent","sd_probrat","mean_probrat_rt",
"valrat_skipped_percent","sd_valrat","mean_valrat_rt",
"earnings","total_experiment_time") #subject-level variables to plot
plot_sub_level_vars(sub_level_data,sl_hists,path_to_project_directory) #create and save plot grids
atcheck_review("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv", min_passed = 3)
source("~/projects/ARL_bv/code/functions/arlbv_utilities.R", echo=TRUE)
atcheck_review("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv", min_passed = 3)
atcheck_review("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv", min_passed = 2)
bp <- atcheck_review("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv", min_passed = 2)
bp <- atcheck_review("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv", min_passed = 2)
prolific_bp("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv",ids=bp)
prolific_bp("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv",ids=bp$approve_ids)
prolific_bp("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv",ids=c(678ff3b08bffecb6f1dbfe49,
approve_ids = c(678ff3b08bffecb6f1dbfe49,
pro <- read.csv("/Users/dp/Downloads/prolific_export_67c9e520b1eef533791f6907.csv")
prolific_bp("/Users/dp/projects/RVA_3/analysis_data/sub_level_data_all_subs_2025-03-06_18_25_38.256832.csv",ids=pro$Participant.id)
length(pro$Participant.id)
library(qualtRics)
library(tidyverse)
library(ddpcr)
library(tidyverse)
library(sigmoid)
##SET MANUALLY
path_to_project_directory <- "~/projects/RVA_3/"
path_to_s22 <- "~/projects/spring_2022_study/"
path_to_s22fu <- "~/projects/s22_follow_up/"
path_to_rva <- "~/projects/RVA/"
ids_to_exclude <- c("6398dc9eee4c333e1712e777") #Ps whose data you don't want to analyze even if it looks good
##############
#clear out results from old analyses
system("mv /Users/dp/projects/RVA_3/analysis_data/*.csv /Users/dp/projects/RVA_3/analysis_data/old_analysis_data") #move any CSV files in this folder to old_analysis_data folder
system("rm -rf /Users/dp/projects/RVA_3/analysis_data/qc_plots/trial_level_vars/*") #clear the folder containing trial-level plots
system("rm -f /Users/dp/projects/RVA_3/analysis_data/qc_plots/sub_level_variables.pdf") #remove old subject level variable plot
#read in necessary functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22fu,"code/functions/s22fu_utilities.R"))
source(paste0(path_to_rva,"code/functions/rva_utilities.R"))
source(paste0(path_to_project_directory,"code/functions/rva3_wrangle_psychopy_data.R"))
csvs_to_analyze <- get_csvs_to_analyze(path_to_project_directory,ids_to_exclude,first_time="2025-03-05") #get the usable CSVs to analyze
all_data <- lapply(csvs_to_analyze, rva3_wrangle_psychopy_data) #reformats each CSV, turning it into a long dataset usable for analysis, and adds all variables of interest that can be created from the raw data alone.
#Returns a list - one element for each subject - where each element is itself a list containing dfs with the trial-level data and subject-level data
trials_list <- lapply(all_data, function(l) l[[1]]) #get a list of the trial-level dfs only
trial_level_data <- do.call(rbind,trials_list) #stack trial dfs into one big df
sub_list <- lapply(all_data, function(l) l[[2]]) #get a list of the subject-level dfs only
sub_level_data <- do.call(rbind,sub_list) #stack them into one big data frame
#append the Yes/No answer from qualtrics to this data
sub_level_data <- qualtrics_append(sub_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
trial_level_data <- qualtrics_append(trial_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
#write both to CSVs
date_time <- Sys.time() %>% chartr(" ","_",.) %>% chartr(":","_",.) #grab the date and time, reformatting ':' and '-' to  '_' so you can label the files with it
write.csv(trial_level_data,paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#write subject-level data
write.csv(sub_level_data,paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#Create plot grids of select variables for quality checking. These are saved to the qc_plots folder
tl_hists <- c("val_rat_rt","prob_rat_rt","val_rat","prob_rat") #trial level variables to plot
plot_trial_level_vars(trial_level_data,tl_hists,path_to_project_directory) #create and save plot grids
sl_hists <- c("answers_incorrect","instruct_keypress",
"feed_check_passed","aff_check_passed","worth_check_passed","att_checks_passed",
"probrat_skipped_percent","sd_probrat","mean_probrat_rt",
"valrat_skipped_percent","sd_valrat","mean_valrat_rt",
"earnings","total_experiment_time") #subject-level variables to plot
plot_sub_level_vars(sub_level_data,sl_hists,path_to_project_directory) #create and save plot grids
library(qualtRics)
library(tidyverse)
library(ddpcr)
library(tidyverse)
library(sigmoid)
##SET MANUALLY
path_to_project_directory <- "~/projects/RVA_3/"
path_to_s22 <- "~/projects/spring_2022_study/"
path_to_s22fu <- "~/projects/s22_follow_up/"
path_to_rva <- "~/projects/RVA/"
ids_to_exclude <- c("6398dc9eee4c333e1712e777") #Ps whose data you don't want to analyze even if it looks good
##############
#clear out results from old analyses
system("mv /Users/dp/projects/RVA_3/analysis_data/*.csv /Users/dp/projects/RVA_3/analysis_data/old_analysis_data") #move any CSV files in this folder to old_analysis_data folder
system("rm -rf /Users/dp/projects/RVA_3/analysis_data/qc_plots/trial_level_vars/*") #clear the folder containing trial-level plots
system("rm -f /Users/dp/projects/RVA_3/analysis_data/qc_plots/sub_level_variables.pdf") #remove old subject level variable plot
#read in necessary functions
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22fu,"code/functions/s22fu_utilities.R"))
source(paste0(path_to_rva,"code/functions/rva_utilities.R"))
source(paste0(path_to_project_directory,"code/functions/rva3_wrangle_psychopy_data.R"))
csvs_to_analyze <- get_csvs_to_analyze(path_to_project_directory,ids_to_exclude,first_time="2025-03-05") #get the usable CSVs to analyze
all_data <- lapply(csvs_to_analyze, rva3_wrangle_psychopy_data) #reformats each CSV, turning it into a long dataset usable for analysis, and adds all variables of interest that can be created from the raw data alone.
#Returns a list - one element for each subject - where each element is itself a list containing dfs with the trial-level data and subject-level data
trials_list <- lapply(all_data, function(l) l[[1]]) #get a list of the trial-level dfs only
trial_level_data <- do.call(rbind,trials_list) #stack trial dfs into one big df
sub_list <- lapply(all_data, function(l) l[[2]]) #get a list of the subject-level dfs only
sub_level_data <- do.call(rbind,sub_list) #stack them into one big data frame
#append the Yes/No answer from qualtrics to this data
sub_level_data <- qualtrics_append(sub_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
trial_level_data <- qualtrics_append(trial_level_data,survey="/Users/dp/Downloads/RVA_3_survey.csv",id_col_qualt="participant",join_cols_qualt=c("Q46","Q40","Q41","Q37","Q25"),join_cols_df=c("sig_emot","age","gender","race","ethnicity"))
#write both to CSVs
date_time <- Sys.time() %>% chartr(" ","_",.) %>% chartr(":","_",.) #grab the date and time, reformatting ':' and '-' to  '_' so you can label the files with it
write.csv(trial_level_data,paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#write subject-level data
write.csv(sub_level_data,paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_",date_time,".csv"),row.names = FALSE)
#Create plot grids of select variables for quality checking. These are saved to the qc_plots folder
tl_hists <- c("val_rat_rt","prob_rat_rt","val_rat","prob_rat") #trial level variables to plot
plot_trial_level_vars(trial_level_data,tl_hists,path_to_project_directory) #create and save plot grids
sl_hists <- c("answers_incorrect","instruct_keypress",
"feed_check_passed","aff_check_passed","worth_check_passed","att_checks_passed",
"probrat_skipped_percent","sd_probrat","mean_probrat_rt",
"valrat_skipped_percent","sd_valrat","mean_valrat_rt",
"earnings","total_experiment_time") #subject-level variables to plot
plot_sub_level_vars(sub_level_data,sl_hists,path_to_project_directory) #create and save plot grids
knitr::opts_chunk$set(echo = TRUE)
options(mc.cores=12)
path_to_project_directory <- "~/projects/RVA_3/"
path_to_s22 <- "~/projects/spring_2022_study/"
path_to_rva <- "~/projects/RVA/"
stan_model_dir<- paste0(path_to_project_directory,"code/stan_models/")
model_out_dir <- paste0(path_to_project_directory,"output/results/stan_model_fits/")
#load custom functions
source(paste0(path_to_s22,"code/functions/fit_stan_model.R"))
source(paste0(path_to_s22,"code/functions/s22_utilities.R"))
source(paste0(path_to_s22,"code/functions/stan_utilities.R"))
source(paste0(path_to_rva,"code/functions/rva_utilities.R"))
source(paste0(path_to_project_directory,"code/functions/rva3_utilities.R"))
library(tidyverse)
library(cmdstanr)
library(bayesplot)
library(abind)
library(sigmoid)
library(posterior)
trials <- read.csv(paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_2025-03-06_19_00_16.812019..csv"))
subs <- read.csv(paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_2025-03-06_19_00_16.812019..csv"))
trials <- read.csv(paste0(path_to_project_directory,"analysis_data/trial_level_data_all_subs_2025-03-06_19_00_16.812019.csv"))
subs <- read.csv(paste0(path_to_project_directory,"analysis_data/sub_level_data_all_subs_2025-03-06_19_00_16.812019.csv"))
sub_hard_fail <- subs %>% filter(att_checks_passed < 3 |
answers_incorrect > 3 |
sd_valrat < .04 |
sd_probrat < .05 |
valrat_skipped_percent > .15 |
probrat_skipped_percent > .15 |
trials_completed < 96)
#count the number of soft QC cutoffs each subject meets
subs$softs <- 0
for(i in 1:nrow(subs)){
subs$softs[i] <- length(which(c(subs$att_checks_passed[i] == 3,subs$answers_incorrect[i] == 3,
subs$valrat_skipped_percent[i] > .10,
subs$sd_probrat[i] < .07, subs$probrat_skipped_percent[i] > .10)))
}
sub_soft_fail <- filter(subs,softs >= 2) #identify those who meet more than 2 soft cutoffs
subs_to_exclude <- unique(c(sub_hard_fail$id,sub_soft_fail$id)) #get subjects who failed either set of criteria
#clean data
trials <- trials %>% filter(!(id %in% subs_to_exclude))
subs <- subs %>% filter(!(id %in% subs_to_exclude))
length(subs_to_exclude)/(nrow(subs)+length(subs_to_exclude)) #get percent excluded
nrow(subs)
length(subs_to_exclude)
38/208
trials <- add_sub_indices(trials) #add subject indices to the df. These will match the indices used in Stan.
trials <- add_probe_number(trials,newcol="vrat_number",val_col="val_rat") #add valence rating number
trials <- add_probe_number(trials,newcol="prat_number",val_col="prob_rat") #add probability rating number
# Create mean-centered block and trial numbers
trials <- mutate(trials, bl_cent = block - mean(block), tr_cent = trial_blk - mean(trial_blk))
# Add fractal indexes
trials <- trials %>% mutate(frac_ix = case_when(
frac_img == 1 ~ 1,
frac_img == 9 ~ 2,
frac_img == 3 ~ 3,
frac_img == 5 ~ 4,
)) %>%
mutate(alt_frac_ix = case_when(
frac_ix == 1 ~ 2,
frac_ix == 2 ~ 1,
frac_ix == 3 ~ 4,
frac_ix == 4 ~ 3
))
sum(subs$sig_emot == "Yes")/nrow(subs)
sum(subs$sig_emot == "Yes")/nrow(subs)
sum(subs$sig_emot == "Yes")
subs$sig_emot
subs$sig_emot == "Yes"
sig_emot <- !is.na(subs$sig_emot)
sig_emot
sig_emot <- !is.na(subs$sig_emot)
sig_emot
sig_emot <- subs$sig_emot[!is.na(subs$sig_emot)]
sig_emot
sum(sig_emot == "Yes")/nrow(subs)
subs$age <- as.numeric(subs$age)
mean(subs$age)
median(subs$age)
sd(subs$age)
subs$age <- as.numeric(subs$age)
mean(subs$age,na.rm=T)
median(subs$age,na.rm=T)
sd(subs$age,na.rm=T)
sum(subs$gender=="Male")
sum(subs$gender=="Male",na.rm=T)
sum(subs$gender=="Female",na.rm=T)
sum(subs$gender=="Non-binary",na.rm=T)
view(head(trials))
nrow(trials)/96
nrow(trials)
prereg$diagnostics
prereg$diagnostics
view(filt_sum(prereg$sum,"mu"))
view(prereg$diagnostics$Rhat)
prereg_eff <- get_draws("prereg_rva3",model_out_dir=model_out_dir,vars=c("wx_mu"))
create_interval_plot(arr = prereg_eff, xmin = -0.02, xmax = .01)
head(prereg_eff)
create_interval_plot(arr = prereg_eff, xmin = -0.02, xmax = .01,names=c("wx_mu[1]","wx_mu[2]","wx_mu[3]","wx_mu[4]"))
create_interval_plot(arr = prereg_eff, xmin = -0.02, xmax = .01,names=c("wx_mu[1]","wx_mu[2]","wx_mu[3]"))
create_interval_plot(arr = prereg_eff, xmin = -0.02, xmax = .01,names=c("wx_mu[3]","wx_mu[2]","wx_mu[1]"))
prereg_eff[,,"wx_mu[2]"] > prereg_eff[,,"wx_mu[3]"]
mean(prereg_eff[,,"wx_mu[2]"] > prereg_eff[,,"wx_mu[3]"])
mean(prereg_eff[,,"wx_mu[2]"] < prereg_eff[,,"wx_mu[3]"])
create_interval_plot(arr = prereg_eff, xmin = -0.02, xmax = .01,names=c("wx_mu[3]","wx_mu[2]","wx_mu[1]"))
prereg_eff_vec_lens <- prereg_eff[,,-"wx_mu[4]"]
prereg_eff_vec_lens <- prereg_eff[,,c("wx_mu[1]","wx_mu[2]","wx_mu[3]"]
prereg_eff_vec_lens <- prereg_eff[,,c("wx_mu[1]","wx_mu[2]","wx_mu[3]")]
v_sh-v_hid <- prereg_eff[,,c("wx_mu[2]")] - prereg_eff[,,c("wx_mu[3]")]
v_sh_v_hid <- prereg_eff[,,c("wx_mu[2]")] - prereg_eff[,,c("wx_mu[3]")]
v_sh_v_hid
eff_dims <- abind(prereg_eff[,,c("wx_mu[1]")],prereg_eff[,,c("wx_mu[2]")] - prereg_eff[,,c("wx_mu[3]")],along=3)
eff_dims
head(eff_dims)
prereg_eff[,,c("wx_mu[2]")] - prereg_eff[,,c("wx_mu[3]")]
vec_ws_prereg <- apply(eff_dims,c(1,2),vec_optim,dvec=dvec_mat,init_pars=c(0,0,0)) #get vector weights
set.seed(7)
rew <- c(1,0)
rpe <- c(0.5,-0.5)
dvec_mat <- cbind(rew,rpe) #turn into matrix
vec_ws_prereg <- apply(eff_dims,c(1,2),vec_optim,dvec=dvec_mat,init_pars=c(0,0,0)) #get vector weights
vec_ws_prereg <- apply(eff_dims,c(1,2),vec_optim,dvec=dvec_mat,init_pars=c(0,0)) #get vector weights
vec_ws_prereg_org <- aperm(vec_ws_prereg,c(2,3,1)) #rearrange dimensions
dimnames(vec_ws_prereg_org)[[3]] <- c("rew","rpe") #name appropriately
create_interval_plot(vec_ws_prereg_org,names = c("mm","rpe","rew"),xmin = -.012, xmax = .1)
create_interval_plot(vec_ws_prereg_org,names = c("rpe","rew"),xmin = -.012, xmax = .1)
create_interval_plot(vec_ws_prereg_org,names = c("rpe","rew"),xmin = -.012, xmax = .01)
vec_ws_prereg_org
head(vec_ws_prereg_org)
vec_ws_prereg_org[,,"rew"] > vec_ws_prereg_org[,,"rpe"]
mean(vec_ws_prereg_org[,,"rew"] > vec_ws_prereg_org[,,"rpe"])
.07^2
.7^2+.7^2
.71^2+.71^2
set.seed(7)
rew <- c(1,0)
rpe <- c(0.71,-0.71)
dvec_mat <- cbind(rew,rpe) #turn into matrix
eff_dims <- abind(prereg_eff[,,c("wx_mu[1]")],prereg_eff[,,c("wx_mu[2]")] - prereg_eff[,,c("wx_mu[3]")],along=3)
vec_ws_prereg <- apply(eff_dims,c(1,2),vec_optim,dvec=dvec_mat,init_pars=c(0,0)) #get vector weights
vec_ws_prereg_org <- aperm(vec_ws_prereg,c(2,3,1)) #rearrange dimensions
dimnames(vec_ws_prereg_org)[[3]] <- c("rew","rpe") #name appropriately
create_interval_plot(vec_ws_prereg_org,names = c("rpe","rew"),xmin = -.012, xmax = .01)
mean(vec_ws_prereg_org[,,"rew"] > vec_ws_prereg_org[,,"rpe"])
quantile(vec_ws_prereg_org[,,"rew"]/vec_ws_prereg_org[,,"rpe"],c(.025,.5,.975))
median(vec_ws_prereg_org[,,"rew"])
median(vec_ws_prereg_org[,,"rpe"])
median(prereg_eff[,,"wx_mu[2]"] - prereg_eff[,,"wx_mu[3]"])
quantile(prereg_eff[,,"wx_mu[2]"] - prereg_eff[,,"wx_mu[3]"],c(.025,.5,.975))
prereg$diagnostics$ESS
view(prereg$diagnostics$ESS)
view(prereg$diagnostics$Rhat)
vp_prereg <- get_draws("prereg_rva3",model_out_dir=model_out_dir,vars=c("val_pred"))
vp_prereg <- get_draws("prereg_rva3",model_out_dir=model_out_dir,vars=c("val_pred"))
pp_prereg <- get_draws("prereg_rva3",model_out_dir=model_out_dir,vars=c("prob_pred"))
ps_prereg <- get_draws("prereg_rva3",model_out_dir=model_out_dir,vars=c("prob_sigma"))
prob_rsq_prereg <- mcmc_rsq(pp_prereg,ps_prereg,print_plot=FALSE)
prob_rsq_prereg$sum['median']
vp_prereg <- get_draws("prereg_rva3",model_out_dir=model_out_dir,vars=c("val_pred"))
vss_prereg <- get_draws("prereg_rva3",model_out_dir=model_out_dir,vars=c("val_sigma_sigma"))
vs_prereg <- apply(vss_prereg,c(1,2,3),half_normal_mean)
val_rsq_prereg <- mcmc_rsq(vp_prereg,vs_prereg,print_plot=FALSE)
val_rsq_prereg$sum[['median']]
view(filt_sum(prereg$sum,"mu"))
-0.0018718597*48
library(sigmoid)
sigmoid(3.6797380191)
sigmoid(2)
sigmoid(5)
sigmoid(3.5)
sigmoid(3.7)
sigmoid(-1.8)
mn_data_prereg <- apply(prereg_eff,c(1,2),function(x) sum(abs(x))) #get the manhattan norms for each data vector by summing the absolute values
comb_array_prereg <- abind(vec_ws_prereg_org,mn_data_prereg,along=3) #staple mn_data to the back of the third dimension of the vector weight array
vec_ws_norm_prereg <- apply(comb_array_prereg,c(1,2),get_ports) #get portions of relationship accounted for
vec_ws_norm_org_prereg <- aperm(vec_ws_norm_prereg,c(2,3,1))
dimnames(vec_ws_norm_org_prereg)[[3]] <- c("rew","rpe","mm","resid") #name meaningfully
set.seed(7)
rew <- c(1,0)
rpe <- c(0.71,-0.71)
dvec_mat <- cbind(rew,rpe) #turn into matrix
eff_dims <- abind(prereg_eff[,,c("wx_mu[1]")],prereg_eff[,,c("wx_mu[2]")] - prereg_eff[,,c("wx_mu[3]")],along=3)
vec_ws_prereg <- apply(eff_dims,c(1,2),vec_optim,dvec=dvec_mat,init_pars=c(0,0)) #get vector weights
vec_ws_prereg_org <- aperm(vec_ws_prereg,c(2,3,1)) #rearrange dimensions
dimnames(vec_ws_prereg_org)[[3]] <- c("rew","rpe") #name appropriately
create_interval_plot(vec_ws_prereg_org,names = c("rpe","rew"),xmin = -.012, xmax = .01)
median(vec_ws_prereg_org[,,"rew"])
median(vec_ws_prereg_org[,,"rpe"])
create_interval_plot(arr = prereg_eff, xmin = -0.02, xmax = .01,names=c("wx_mu[3]","wx_mu[2]","wx_mu[1]"))
